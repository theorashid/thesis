# Some notes on the data and the models {#sec-Chapter3}

## Counts of the dead

This thesis is primarily concerned with modelling death rates for small areas in England.
This requires two data sources: counts of deaths, and populations counts.
The dataset is de-identified civil registration data for all deaths in England from 2002 to 2019.
In other words, every death in England from 2002 to 2019.

The data is extracted from the Office for National Statistics (ONS) database and held by SAHSU, which has the required security protocols as individual death records are identifiable data.
The data are updated every year and are mostly complete for previous years, but a handful of deaths are registered in later extracts if the ONS have been waiting on coroner's report to identify underlying cause of death.

Each record comes with information on postcode of residence, allowing us to assign each death into a spatial unit for analysis.
For eacha anlysis, deaths were stratified into the following age groups: 0, 1–4, 5–9, 10–14, then 5-year age groups up to 80–84, and 85 years and older.
There are also a series of ICD-10 (International Classification of Dieases, Tenth Revision) codes from the death certificate associated with the underlying and contributory causes leading to the death.
Here, we focus only on the underlying cause of death, which has been assigned using selection algorithms to improve consistency between doctors [@UserGuideMortality2022].

![Age-specific death rates for broad age groups and life expectancy in England from 2002 to 2019.](../thesis-analysis/thesis_analysis/eda/figures/age_mx.pdf)

Before to fitting any models, it's good practice to explore the data visually, in this case looking at how total mortality varies over different cross sections: sex, age, space, time.

![District-level life expectancy for total deaths from 2002 to 2019.](../thesis-analysis/thesis_analysis/eda/figures/e0_map_LAD_2002_2019.pdf){#fig-ch-3-district}

![Age-specific death rates for England based on total deaths from 2002 to 2019.](../thesis-analysis/thesis_analysis/eda/figures/age_mx.pdf)

![Age-specific death rates for broad age groups and life expectancy in England from 2002 to 2019.](../thesis-analysis/thesis_analysis/eda/figures/age_mx_time.pdf)

Look at different cross sections, slice by age, space and time @fig-ch-3-district
By age + sex, log scale, death rates in 2019
Colour geom_tile plot age-specific death rates over time, trans = log
Life expectancy male and female over time
e0 aggregated 2002-19 by district

Here, we have taken slices, but the aim is to calculate death rates for each sex-age-space-time unit

### Geographies of England

Having already introduced the term "district" in Figure XXs, I'll briefly show the lay of the land in terms of geographies used in this thesis.
This thesis is concerned only with England, as Scotland, Wales, and Northern Ireland have their own separate deprivation data which are not comparable.
The geographies used here in England form a nested hierarchy of spatial units from regions, districts, Middle-layer Super Output Areas (MSOAs), Lower-layer Super Output Areas (LSOAs).
The numbers of each of the units are summarised in @tbl-geography.

| Geography | Number of units | Median (5th - 9th percentile) population in 2019 |
| --------- | --------------- | ------------------------------------------------ |
| region    | 9               |                                                  |
| district  | 314             |                                                  |
| MSOA      | 6791            | 7985 (5760–11917)                                |
| LSOA      | 32844           |                                                  |

: The different geographical units of England used in thesis and their numbers. {#tbl-geography}

England is divided into 9 regions (London, North West, West Midlands, etc).
Within these regions, there are 314 local authority districts.
Districts are administative geographies formed from a mixture of London boroughs, metropolitan and non-metropolitan distrcts, and unitary autorities.
They are responsible for local policies, and therefore subject to local government restructuring and bounary changes.
For stability, we chose the district boundaries from 2020.

LSOAs are a type of census geography made up of around four or five smaller units called Output Areas (OAs).
OAs are the smallest building block for census statistics, with between 40 and 250 households and typically 100 to 625 people, and are designed to have some socioeconomic homogeneity.
MSOAs are then comprised of around four or five LSOAs, and these MSOAs fit within district boundaries.
OAs, LSOAs, and MSOAs are all statisitcal units are designed by the ONS purely for analysis purposes, so researchers can use spatial units with similar, but small, population sizes.
No policies are created using these units [@2011CensusGeographies2022].

### Counts of the living

This second data sources we require are populations counts.
These are taken from mid-year population estimates of the usual resident population by the ONS [@MiddleLayerSuper2021; @LowerLayerSuper2021].
The ONS estimates inter-censal populations on a rolling basis, updating the previous year's value using the change in the population in the GP patient registration data as an indicator of the true population change.
The LSOA populations are fully consistent with estimates for higher levels in the nested geographical hierarchical including MSOAs, districts, regions and the national total for England [@PopulationEstimatesOutput2021].

### Deprivation data

We used data for the following measures of socioeconomic deprivation from the English Indices of Deprivation:

- Income deprivation (referred to as _poverty_ hereafter). The proportion of the geographical population claiming income-related benefits due to being out of work or having low earnings.
- Employment deprivation (referred to as _unemployment_ hereafter). The proportion of the relevant population of the geography involuntarily excluded from the labour market due to unemployment, sickness or disability, or caring responsibilities.
- Education, skills and training deprivation (referred to as _low education_ hereafter). Lack of attainment and skills, including education attainment levels, school attendance, and language proficiency indicators in the geographical population.

The above measures are the three largest contributors to the Index of Multiple Deprivation (IMD), excluding a domain on health that also uses mortality data.
The data are produced at the LSOA level [@EnglishIndicesDeprivation2019].

IMD data are not available for every year.
The analysis period for the thesis is 2002 to 2019, so we used data for these measures for 2004, as data for 2002 were not available, and 2019.
The 2004 data on deprivation domains were reported for LSOA boundaries from the 2001 census.
We mapped these data to the 2011 census LSOA boundaries by assigning the 2001 LSOA score to all postcodes contained within it, then overlaying the 2011 LSOA boundaries, and averaging the score for all constituent postcodes of each LSOA, to obtain the corresponding score for each 2011 LSOA.

The definition of the indicators can change over time.
Further, the indicator used for measuring education, skills and training deprivation (low education) is not directly interpretable because it combines multiple concepts cannot be simply expressed as a proportion of the population.
Therefore, we used ranking rather than scores so that comparisons can be made not only across spatial units in a single year, but also across the different years.

The data for geographies larger than LSOAs in @tbl-geography were created by ranking the population-weighted average of scores for all constituent LSOAs, as done previously for districts [@EnglishIndicesDeprivation2019].

### Migration data

linked migration data @vandijkUsingLinkedConsumer2021

## Modelling the dead

For each chapter, the quantity of interest is the same: mortality in each age group, spatial unit and year.
Empirically, death rates can be calculated from observed data as the number of deaths divided by the population in each strata.
Formally, using $a$, $s$, and $t$ to index age, spatial unit and time respectively, we write
$$
\hat{m}_{ast} = \frac{\text{deaths}_{ast}}{\text{population}_{ast}},
$$ {#eq-death-rate}
where $\hat{m}_{ast}$ is the death rate.
When the number of deaths becomes small, however, the empirical death rate presents an apparent variability from year to year, or from spatial unit to spatial unit, which is larger than the true differences in the risk of death.
The problem is exarcerbated for young ages or rare diseases, where the number of deaths might be zero, or for smaller geographical units, where the population might be zero.
Thus, we use Bayesian hierarchical models to obtain stable estimates of death rates by sharing information across age groups, spatial units, and years.
An added advantage of the Bayesian paradigm is the robust error estimates.

This is a regression task.
We simply want to smooth over the data – the models aren't being used for prediction.
We tried to design as complex a model as possible, to capture as much of the true variation in the data as possible using epidemiologcial knowledge to choose plausible effects.
In other words, the model is "full", enough paramaters to capture all the true variability.
Models are overspecified, like Bayesian neural networks (AGW paper)
The downside of this approach is that more parameters is harder to fit, and fewer parameters, or parsimonious models, makes Bayesian inference easier.

## Inference

The decision was made early in the PhD to use Markov chain Monte Carlo (MCMC) sampling methods for inference, as this is the "gold standard" with guarantees that the sequence of samples will asmyptotically converge to the true posterior.
Furthermore, the state-of-the-art approximate inference package for spatial models, INLA, scales badly with the number of effects, and hence would struggle with the models of dimensionality in this thesis.

Bayesian models are specified in a probabilistic programming language.
The starting point for this project was the `NIMBLE` package [@devalpineNIMBLEMCMCParticle2022; @devalpineProgrammingModelsWriting2017].
`NIMBLE` uses the BUGS ("Bayesian inference Using Gibbs Sampling") syntax for defining a hierarchical model, which the group has a lot of experience with, as `WinBUGS`, one of the earliest software packages for Bayesian analysis, was developed largely in the department for use on SAHSU studies.
`NIMBLE` has an `R` interface but compiles models to `C++` for speed and scalability.
It also increases the efficiency of Gibbs sampling efficiently by automatically finding conjugate relationships between parameters in the model and marginalising over them wherever possible.
An added advantage is that the group has a close relationship with the lead developer of `NIMBLE`.

Nevertheless, Bayesian inference is difficult to scale, and some of the models in this thesis had in excess of $10^6$ parameters and took `NIMBLE` between 10 and 14 days to collect enough samples of the posterior.
One of the main issues with `NIMBLE` was that the vast majority of the parameters in the model could not exploit efficient conjugate samplers, and instead used variants of basic Metropolis-Hastings samplers, which, despite numerous efforts at tuning, were inefficient.
Although `NIMBLE` could execute a reasonable number of samples per second, the MCMC chains were struggling to explore the posterior quickly so the _effective_ sample size per second was low.
This is a common problem in spatial and spatiotemporal models, where the parameters are correlated by design.
To overcome these mixing issues, the chains had to be run for longer and thinned (i.e. take every $n^{\text{th}}$ sample so the Markov chain samples are closer to independent).

I spent a lot of time trying different probabilsitic programming languages across `R`, `python` and `Julia`, in particular packages that implemented the more efficient No U-Turn Sampler (NUTS) [@rashidProbabilisticprogrammingpackages2022].
In the end, I settled on `NumPyro` [@phanComposableEffectsFlexible2019] because it was the fastest and inference could be performed on a GPU, rather than CPUs, which is faster for large models [@laoTfpMcmcModern2020].
The major downside was that `NumPyro` had not been used extensively by the spatial community, so I had to implement the CAR distribution from @eq-CAR-prec, which has since been contributed to the source code.
Rewriting the model in `NumPyro` and sampling on a GPU cut the required runtime down to around a day.
`NumPyro` also has built-in methods for approximate variational inference, such as the Laplace approximation, but these failed to converge to sensible values for these models without heavy customisation of variational function, so I stuck with sampling methods.

## Clean code and open source

I am strong believer in open source science, and I have put a lot of attention into open sourcing the code for all analyses during the PhD.
With open science, not only do we faciliate the scientific method as our process and results are reviewed scientific method, but we also allow people in the future to easily reuse and build on our models.
It can also generate interest from researchers in different fields using similar models and from developers looking to challenge their software on complicated research questions, both of which I have seen first-hand during the course of my studies.
The code is clean, version-controlled and follows best practices for scientific software engineering wherever possible.
As well as code contributed to open source projects along the way, the code for [statistical models](https://github.com/theorashid/mortality-statsmodel), [plots and analysis](https://github.com/theorashid/thesis-analysis), and the [thesis istelf](https://github.com/theorashid/thesis) can be found on GitHub.
